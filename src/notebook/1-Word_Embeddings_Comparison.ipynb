{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d94cc27",
   "metadata": {},
   "source": [
    "\n",
    "# Comparing Word Embedding Approaches: From One-Hot Encoding to OpenAI Embeddings\n",
    "\n",
    "This notebook demonstrates and compares different word embedding approaches, ranging from simple one-hot encoding to advanced embeddings like OpenAI's.\n",
    "\n",
    "## Objectives\n",
    "1. Understand the evolution of word embeddings.\n",
    "2. Implement and compare:\n",
    "   - One-Hot Encoding\n",
    "   - Word2Vec\n",
    "   - GloVe\n",
    "   - BERT (contextual embeddings)\n",
    "   - OpenAI Embeddings\n",
    "3. Evaluate the embeddings on a semantic similarity task.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b922333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim\n",
    "%pip install transformers\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import pipeline\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Deep learning is a key technology for AI\",\n",
    "    \"Word embeddings capture semantic meaning\",\n",
    "    \"OpenAI embeddings are state-of-the-art\",\n",
    "    \"Machine learning is evolving rapidly\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences into words for embedding methods\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b82d0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. One-Hot Encoding\n",
    "\n",
    "One-hot encoding represents each word as a unique binary vector. However, it does not capture any semantic relationships between words.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-Hot Encoding\n",
    "vocabulary = sorted(set(word for sentence in tokenized_sentences for word in sentence))\n",
    "one_hot_vectors = {word: np.eye(len(vocabulary))[i] for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Display one-hot encoding for a few words\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"One-Hot Encoding Example:\", one_hot_vectors['learning'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76540280",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Word2Vec\n",
    "\n",
    "Word2Vec learns embeddings by predicting the context of a word within a sliding window. It captures semantic relationships like synonyms.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=10, window=2, min_count=1, workers=1, sg=1)\n",
    "word2vec_vectors = {word: word2vec_model.wv[word] for word in vocabulary}\n",
    "\n",
    "# Display Word2Vec embeddings for a few words\n",
    "print(\"Word2Vec Embedding Example (learning):\", word2vec_vectors['learning'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf738b",
   "metadata": {},
   "source": [
    "\n",
    "## 3. GloVe\n",
    "\n",
    "GloVe uses matrix factorization to learn embeddings that capture global co-occurrence statistics of words in a corpus.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb8261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulating GloVe by loading pre-trained vectors (example placeholder)\n",
    "# In practice, you can download GloVe vectors and load them here\n",
    "glove_vectors = {word: np.random.rand(10) for word in vocabulary}\n",
    "print(\"GloVe Embedding Example (learning):\", glove_vectors['learning'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84377127",
   "metadata": {},
   "source": [
    "\n",
    "## 4. BERT (Contextual Embeddings)\n",
    "\n",
    "BERT generates contextual embeddings, meaning the same word can have different embeddings depending on its context.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use BERT for embeddings\n",
    "bert_pipeline = pipeline('feature-extraction', model='bert-base-uncased', tokenizer='bert-base-uncased')\n",
    "\n",
    "# Generate embeddings for a sentence\n",
    "bert_embedding = bert_pipeline(\"Deep learning is a key technology for AI\")[0]\n",
    "print(\"BERT Embedding Shape (first word):\", np.array(bert_embedding[1]).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5f01e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. OpenAI Embeddings\n",
    "\n",
    "OpenAI embeddings provide state-of-the-art representations using their advanced models.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder for OpenAI embeddings (replace with actual API usage)\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version = \"2024-02-01\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "response = client.embeddings.create(input=\"Deep learning is a key technology for AI\", model=\"demo-cosmos-rag-emb\")\n",
    "openai_embedding = response.data[0].embedding\n",
    "print(\"OpenAI Embedding Example (simulated):\", openai_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa951a76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922a0a21",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "This notebook demonstrates the progression of word embedding techniques, highlighting their strengths and limitations. Advanced embeddings like OpenAI's provide state-of-the-art representations, but simpler methods like Word2Vec are still useful for certain tasks.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
