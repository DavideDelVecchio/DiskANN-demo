{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f7d85e",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding Vector Search Techniques: iVFlat, HNSW, and DiskANN\n",
    "\n",
    "This notebook provides an overview and comparison of three popular methods used in vector search: **iVFlat**, **HNSW (Hierarchical Navigable Small World)**, and **DiskANN**. We will explore their characteristics, strengths, and appropriate use cases.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52852403",
   "metadata": {},
   "source": [
    "\n",
    "## 1. What is Vector Search?\n",
    "\n",
    "Vector search refers to finding similar vectors in a high-dimensional space. It is commonly used in applications such as:\n",
    "\n",
    "- **Recommendation Systems**: Suggesting items based on similarity.\n",
    "- **Semantic Search**: Retrieving results based on meaning rather than exact text match.\n",
    "- **Image/Video Retrieval**: Finding similar multimedia content.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0be10f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Overview of Techniques\n",
    "\n",
    "### 2.1 iVFlat (Inverted Flat Index)\n",
    "- **Description**: Combines an inverted index with flat vector storage.\n",
    "- **Advantages**:\n",
    "  - Simple and efficient for smaller datasets.\n",
    "  - Exact search is possible.\n",
    "- **Limitations**:\n",
    "  - Requires significant memory for large datasets.\n",
    "  - Slower query times compared to approximate methods.\n",
    "\n",
    "### 2.2 HNSW (Hierarchical Navigable Small World)\n",
    "- **Description**: Graph-based approach to approximate nearest neighbor (ANN) search.\n",
    "- **Advantages**:\n",
    "  - Fast query times.\n",
    "  - Good balance of accuracy and speed.\n",
    "- **Limitations**:\n",
    "  - High memory usage.\n",
    "  - Build times can be significant for large datasets.\n",
    "\n",
    "### 2.3 DiskANN (Disk-friendly ANN)\n",
    "- **Description**: Optimized for datasets larger than RAM capacity; uses disk-based storage efficiently.\n",
    "- **Advantages**:\n",
    "  - Handles extremely large datasets by leveraging SSDs.\n",
    "  - Low memory footprint.\n",
    "- **Limitations**:\n",
    "  - Slower than in-memory methods.\n",
    "  - SSD performance can be a bottleneck.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3346e92",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Comparison of Techniques\n",
    "\n",
    "| Feature                  | iVFlat            | HNSW                     | DiskANN              |\n",
    "|--------------------------|-------------------|--------------------------|----------------------|\n",
    "| **Data Structure**       | Inverted index    | Graph-based              | Disk + RAM           |\n",
    "| **Memory Usage**         | High              | Medium-High              | Low (RAM) + Disk     |\n",
    "| **Query Time Complexity**| Linear            | Logarithmic              | Logarithmic          |\n",
    "| **Build Time Complexity**| Low               | High                     | Medium               |\n",
    "| **Scalability**          | Poor (RAM bound)  | Medium                   | Excellent (disk support)|\n",
    "| **Best Use Cases**       | Small datasets    | Fast in-memory search    | Large-scale datasets |\n",
    "\n",
    "### Key Considerations\n",
    "- Use **iVFlat** for simple and small datasets.\n",
    "- Choose **HNSW** for high-speed, in-memory searches with high accuracy.\n",
    "- Opt for **DiskANN** when working with datasets exceeding RAM capacity.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Using HNSW for Vector Search with Faiss\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Generate random data\n",
    "d = 128  # Dimension\n",
    "nb = 10000  # Database size\n",
    "nq = 5  # Number of queries\n",
    "\n",
    "np.random.seed(1234)\n",
    "database = np.random.random((nb, d)).astype('float32')\n",
    "queries = np.random.random((nq, d)).astype('float32')\n",
    "\n",
    "# Build index\n",
    "index = faiss.IndexHNSWFlat(d, 32)  # 32 is the number of neighbors\n",
    "index.add(database)\n",
    "\n",
    "# Perform search\n",
    "k = 4  # Number of nearest neighbors\n",
    "distances, indices = index.search(queries, k)\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Distances to nearest neighbors:\", distances)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c082000",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "Each method has its strengths and weaknesses. The choice of method depends on the size of your dataset, available memory, and query time requirements:\n",
    "\n",
    "- **iVFlat**: Best for small, simple datasets.\n",
    "- **HNSW**: Excellent for fast in-memory approximate search.\n",
    "- **DiskANN**: Ideal for massive datasets that exceed RAM capacity.\n",
    "\n",
    "Feel free to explore the provided code to understand how HNSW works in practice!\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
